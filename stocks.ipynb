{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import constants\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=constants.SUBSCRIPTION_ID,\n",
    "    resource_group_name=constants.RESOURCE_GROUP_NAME,\n",
    "    workspace_name=constants.WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all versions will have the date encoded in the version number\n",
    "version_num = datetime.datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/get_data.py\n",
    "\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", type=str, help=\"path or URL to input data\")\n",
    "    parser.add_argument(\"--train_data\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.input_data)\n",
    "    df = df.drop(\"adjusted_close\", axis=1)\n",
    "\n",
    "    df = prep_data(df)\n",
    "    df = df.fillna(method=\"ffill\")\n",
    "\n",
    "    path = os.path.join(args.train_data, \"stock-data.csv\")\n",
    "    df.to_csv(path)\n",
    "\n",
    "    stock_data = Data(\n",
    "        name=\"stock-data\",\n",
    "        path=path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Dataset to train a model on the IBM stock data.\",\n",
    "        tags={\"source_type\": \"web\", \"source\": \"AlphaVantage\"},\n",
    "    )\n",
    "\n",
    "    credit_data = ml_client.data.create_or_update(stock_data)\n",
    "\n",
    "def prep_data(dataframe):\n",
    "    # get rolling mean an exponential moving average\n",
    "    dataframe[\"rolling_3_mean\"] = dataframe[\"close\"].shift(1).rolling(3).mean()\n",
    "    dataframe[\"rolling_7_mean\"] = dataframe[\"close\"].shift(1).rolling(3).mean()\n",
    "    dataframe[\"ewma\"] = dataframe[\"close\"].shift(1).ewm(alpha=0.5).mean()\n",
    "\n",
    "    # convert timestamp to unix timecode \n",
    "    dataframe['unix_timestamp'] = pd.to_datetime(dataframe['timestamp']).values.astype(int)/ 10**9\n",
    "\n",
    "    # get day of week and month\n",
    "    dataframe[\"timestamp\"] = pd.to_datetime(dataframe[\"timestamp\"])\n",
    "    dataframe[\"weekday\"] = dataframe['timestamp'].dt.dayofweek\n",
    "    dataframe[\"month\"] = dataframe['timestamp'].dt.month\n",
    "    dataframe = dataframe.drop(\"timestamp\", axis=1)\n",
    "\n",
    "    # shift the target column \n",
    "    dataframe[\"close_shifted\"] = dataframe[\"close\"].shift(-1)\n",
    "    return dataframe\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/train_model.py\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "import lightgbm as lgbm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to input data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Get a handle to the workspace\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "    # all versions will have the date encoded in the version number\n",
    "    version_num = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    df = pd.read_csv(args.train_data)\n",
    "    df.head()\n",
    "\n",
    "    X = np.array(df.drop([\"Unnamed: 0\", \"close\", \"close_shifted\"], axis=1))\n",
    "    y = np.array(df[\"close_shifted\"])\n",
    "\n",
    "    # Train model on whole dataset\n",
    "    lgbm_model = lgbm.LGBMRegressor(max_depth=100, reg_alpha=0.05, reg_lambda=0.05).fit(X, y)\n",
    "\n",
    "    with open('ibm_model.pkl', 'wb') as f:\n",
    "        pickle.dump(lgbm_model, f)\n",
    "\n",
    "    file_model = Model(\n",
    "        path=\"ibm_model.pkl\",\n",
    "        type=AssetTypes.CUSTOM_MODEL,\n",
    "        name=\"IBM-Model\",\n",
    "        description=\"Model created from local file.\",\n",
    "        version=version_num\n",
    "    )\n",
    "    ml_client.models.create_or_update(file_model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile dependencies/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - numpy\n",
    "  - pip\n",
    "  - lightgbm\n",
    "  - pandas\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]\n",
    "    - xlrd\n",
    "    - mlflow\n",
    "    - azureml-mlflow\n",
    "    - azure-ai-ml\n",
    "    - azureml-fsspec \n",
    "    - mltable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env_name = \"stock-training-env\"\n",
    "\n",
    "try:    \n",
    "    pipeline_job_env = ml_client.environments.get(custom_env_name, version=\"1.5\")\n",
    "\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for stock training pipeline\",\n",
    "        tags={\"lightgbm\": \"3.3.3\"},\n",
    "        conda_file=os.path.join(\"dependencies\", \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/curated/lightgbm-3.2-ubuntu18.04-py37-cpu:48\",\n",
    "        version=\"1.5\",\n",
    "    )\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Loads data via AlphaVantage API input, preps data and stores to as data asset\",\n",
    "    inputs={\"input_data\": Input(type=\"uri_file\")},\n",
    "    outputs={\"train_data\": Output(type=\"uri_folder\")},\n",
    "    code=\"./components/get_data.py\",\n",
    "    command=\"python get_data.py --input_data ${{inputs.input_data}} --train_data ${{outputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_component = command(\n",
    "    name=\"train_model\",\n",
    "    display_name=\"Model training with data from previous step\",\n",
    "    description=\"Trains a LightGBM model with preprocessed data\",\n",
    "    inputs={\"train_data\": Input(type=\"uri_folder\")},\n",
    "    code=\"./components/train_model.py\",\n",
    "    command=\"python train_model.py --train_data ${{inputs.train_data}}\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    "    compute=\"ava\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': 'Succeeded', 'provisioning_errors': None, 'name': 'ava', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/MlGroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace/computes/ava', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\Leo\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x00000245A243F100>, 'resource_id': None, 'location': 'northeurope', 'size': 'STANDARD_A1_V2', 'min_instances': 0, 'max_instances': 1, 'idle_time_before_scale_down': 180.0, 'identity': None, 'ssh_public_access_enabled': True, 'ssh_settings': None, 'network_settings': <azure.ai.ml.entities._compute.compute.NetworkSettings object at 0x0000024582230910>, 'tier': 'low_priority', 'subnet': None})\n"
     ]
    }
   ],
   "source": [
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"ava\"\n",
    "print(ml_client.compute.get(cluster_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from components import get_data, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(compute=\"ava\")\n",
    "def pipeline_with_non_python_components(input_data):\n",
    "\n",
    "    data_prep_job = data_prep_component(input_data=input_data)\n",
    "    train_model_job = train_model_component(train_data=data_prep_job.outputs.train_data) # feed putput of previous step into the training job\n",
    "\n",
    "    return {\"out\": data_prep_job.outputs.train_data}\n",
    "\n",
    "\n",
    "pipeline_job = pipeline_with_non_python_components(\n",
    "    input_data=Input(\n",
    "        path=\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=IBMdatatype=csv&outputsize=full&apikey=SGXL42YQBJ7R7WXL\"\n",
    "        ) # stock data via AlphaVantage\n",
    "    )\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"ava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>stock-training-pipeline</td><td>patient_oxygen_wx0yf5prjr</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/patient_oxygen_wx0yf5prjr?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace&amp;tid=08548f02-0216-4325-938b-fd30f6829e55\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x00000245CA78A220>}, 'outputs': {'out': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x00000245CA78A940>}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x00000245CA78AF40>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'pipeline_with_non_python_components', 'is_deterministic': None, 'inputs': {'input_data': {}}, 'outputs': {'out': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'data_prep_job': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'data_prep_job', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x00000245CA288280>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Data preparation for training', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.input_data}}'}, 'job_outputs': {'train_data': '${{parent.outputs.out}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x00000245CA63CC70>}, 'outputs': {'train_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x00000245CA63CF40>}, 'component': 'azureml_anonymous:5293df14-f3f6-46bd-b081-8feec204500f', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'dd87763e-cfa7-447a-a48d-e6ae904f7914', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'train_model_job': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'train_model_job', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x00000245CA63C940>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Model training with data from previous step', 'experiment_name': None, 'compute': 'ava', 'services': None, 'comment': None, 'job_inputs': {'train_data': '${{parent.jobs.data_prep_job.outputs.train_data}}'}, 'job_outputs': {}, 'inputs': {'train_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x00000245CA63CDF0>}, 'outputs': {}, 'component': 'azureml_anonymous:cef4254b-90ce-4fc2-bc03-70a702b8bd55', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'a5968782-f205-4fdc-89a1-34d62cf913b3', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'patient_oxygen_wx0yf5prjr', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/LeonardPuettmann/Chatbot.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '0bf6443394c3f5471b9d07d70019c3c5d28e29bc', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.defaultComputeName': 'ava', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourceGroups/MlGroup/providers/Microsoft.MachineLearningServices/workspaces/mlworkspace/jobs/patient_oxygen_wx0yf5prjr', 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\Leo\\\\OneDrive\\\\Programming\\\\Python\\\\azure\\\\stock-pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x00000245CA78A0A0>, 'serialize': <msrest.serialization.Serializer object at 0x00000245CA78A4C0>, 'display_name': 'pipeline_with_non_python_components', 'experiment_name': 'stock-training-pipeline', 'compute': 'ava', 'services': {'Tracking': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x00000245CA78A3D0>, 'Studio': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x00000245CA78A580>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"stock-training-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: patient_oxygen_wx0yf5prjr\n",
      "Web View: https://ml.azure.com/runs/patient_oxygen_wx0yf5prjr?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2023-01-01 15:41:38Z] Submitting 1 runs, first five are: f597c5ec:b86666db-10bb-4ccf-b020-7a93ef9ae6da\n",
      "[2023-01-01 15:44:51Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: patient_oxygen_wx0yf5prjr\n",
      "Web View: https://ml.azure.com/runs/patient_oxygen_wx0yf5prjr?wsid=/subscriptions/5a361d37-b562-4eee-981b-0936493063e9/resourcegroups/MlGroup/workspaces/mlworkspace\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has some failed steps. See child run or execution logs for more details.\",\n        \"message_format\": \"Pipeline has some failed steps. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"northeurope\",\n    \"location\": \"northeurope\",\n    \"time\": \"2023-01-01T15:44:51.433546Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Wait until the job completes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mstream(pipeline_job\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:259\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    258\u001b[0m     \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 259\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:617\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[39mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[0;32m    615\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[39m=\u001b[39mjob_object\u001b[39m.\u001b[39mid)\n\u001b[1;32m--> 617\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_logs_until_completion(\n\u001b[0;32m    618\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runs_operations, job_object, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datastore_operations, requests_pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requests_pipeline\n\u001b[0;32m    619\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:297\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    295\u001b[0m         file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m         \u001b[39mraise\u001b[39;00m JobException(\n\u001b[0;32m    298\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(json\u001b[39m.\u001b[39mdumps(error, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)),\n\u001b[0;32m    299\u001b[0m             target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[0;32m    300\u001b[0m             no_personal_data_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mException raised on failed job.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    301\u001b[0m             error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mSYSTEM_ERROR,\n\u001b[0;32m    302\u001b[0m         )\n\u001b[0;32m    304\u001b[0m file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    305\u001b[0m file_handle\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has some failed steps. See child run or execution logs for more details.\",\n        \"message_format\": \"Pipeline has some failed steps. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"northeurope\",\n    \"location\": \"northeurope\",\n    \"time\": \"2023-01-01T15:44:51.433546Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d37735ab87abccc8139abe988438c739cb03a9d8c6c58871ebffce8beac18701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
